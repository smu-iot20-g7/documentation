<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Tablevision: Tray Returns Monitoring with AI | CS462: G7 IoT Documentation</title>
    <meta name="generator" content="VuePress 1.5.2">
    
    <meta name="description" content="Documentation for our RFID and Tablevision IoT Solution for Beo Crescent">
    <meta name="theme-color" content="#3eaf7c">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="preload" href="/assets/css/0.styles.64f42e8e.css" as="style"><link rel="preload" href="/assets/js/app.438a13a4.js" as="script"><link rel="preload" href="/assets/js/2.8c709f75.js" as="script"><link rel="preload" href="/assets/js/4.31391edf.js" as="script"><link rel="prefetch" href="/assets/js/10.f519e4a9.js"><link rel="prefetch" href="/assets/js/3.eb2d0fe1.js"><link rel="prefetch" href="/assets/js/5.e10f5490.js"><link rel="prefetch" href="/assets/js/6.7e06a402.js"><link rel="prefetch" href="/assets/js/7.8b646afd.js"><link rel="prefetch" href="/assets/js/8.4f0645e9.js"><link rel="prefetch" href="/assets/js/9.8e4ae218.js">
    <link rel="stylesheet" href="/assets/css/0.styles.64f42e8e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">CS462: G7 IoT Documentation</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/journal/" class="nav-link router-link-active">
  Journal
</a></div><div class="nav-item"><a href="/fsr-rfid/" class="nav-link">
  FSR-RFID
</a></div><div class="nav-item"><a href="/tablevision/" class="nav-link">
  Tablevision
</a></div><div class="nav-item"><a href="https://v1.vuepress.vuejs.org" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Dashboard
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/journal/" class="nav-link router-link-active">
  Journal
</a></div><div class="nav-item"><a href="/fsr-rfid/" class="nav-link">
  FSR-RFID
</a></div><div class="nav-item"><a href="/tablevision/" class="nav-link">
  Tablevision
</a></div><div class="nav-item"><a href="https://v1.vuepress.vuejs.org" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Dashboard
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Journal</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/journal/" aria-current="page" class="sidebar-link">Hawker centre dilemma: &quot;Must return trays meh?&quot;</a></li><li><a href="/journal/fsr-rfid.html" class="sidebar-link">Round Force Sensor + RFID Tray Tagging</a></li><li><a href="/journal/tablevision.html" aria-current="page" class="active sidebar-link">Tablevision: Tray Returns Monitoring with AI</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/journal/tablevision.html#architecture-diagram" class="sidebar-link">Architecture Diagram</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#overview" class="sidebar-link">Overview</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#training-our-machine-learning-model" class="sidebar-link">Training our Machine Learning Model</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#initialising-tablevision" class="sidebar-link">Initialising Tablevision</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#sensor-modalities" class="sidebar-link">Sensor modalities</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#data-insights-and-accuracy" class="sidebar-link">Data, insights and accuracy</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#limitations" class="sidebar-link">Limitations</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#key-benefits-of-tablevision" class="sidebar-link">Key Benefits of Tablevision</a></li><li class="sidebar-sub-header"><a href="/journal/tablevision.html#costs" class="sidebar-link">Costs</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="tablevision-tray-returns-monitoring-with-ai"><a href="#tablevision-tray-returns-monitoring-with-ai" class="header-anchor">#</a> Tablevision: Tray Returns Monitoring with AI</h1> <h2 id="architecture-diagram"><a href="#architecture-diagram" class="header-anchor">#</a> Architecture Diagram</h2> <p><img src="/assets/img/tablevision.81a39ea1.png" alt="diagram"></p> <h2 id="overview"><a href="#overview" class="header-anchor">#</a> Overview</h2> <div class="custom-block tip"><p class="custom-block-title">NOTE</p> <p>The term &quot;tray returns&quot; is our umbrella term that refer to hawker centre trays or crockeries (i.e. bowls, cutleries, plates) returned to the Tray Return Point.</p></div> <p>Our solution incorporates an 8-megapixel camera sensor, connected to a sensor gateway. In our case, the gateway is our Raspberry Pi. It is designed to be able to automate the detection of self-initiated tray returns – or the reverse of tray returns by cleaners, through a custom machine learning model training.</p> <p>Through Tablevision, we will be monitoring:</p> <ul><li><strong>Negative tray return rates</strong>: a measure of tray returns by the cleaners – hence the term &quot;negative&quot; which indicates negative behaviour by patrons for not clearing trays, and;</li> <li><strong>Positive tray return rates</strong>: a measure of tray self-returns by patrons – the term &quot;positive&quot; indicates the positive behaviour of patrons clearing up after eating</li></ul> <h2 id="training-our-machine-learning-model"><a href="#training-our-machine-learning-model" class="header-anchor">#</a> Training our Machine Learning Model</h2> <p>Initially, we used the <a href="https://www.clarifai.com/" target="_blank" rel="noopener noreferrer">Clarif.ai API<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> for its object detection API. However, the free version is only available for 1,000 API calls, which was insufficient for our use case. Furthermore, we needed a pay-as-you-go (PAYG) model that enables us to have a fine-grained billing for our object detection, and Clarif.ai's pricing model only indicates prices per block of API calls.</p> <p><a href="https://cloud.google.com/vision" target="_blank" rel="noopener noreferrer">Google Cloud Vision AI<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> comes in as our next reliable alternative. It enables us to derive and detect objects from the image captured with our camera modules, and it comes with a pay-as-you-go model.</p> <p>We decided to give it a try.</p> <p>And that's when we realised that Google had generously given us free $300 credits for model training and image detection, on top of the $300 credit already given as a promotion to use any of Google Cloud's services. Such a great way to let us students experiment with Cloud technologies.</p> <p>To train our model, we used a series of images that we captured from our installed camera module atop some tables, such as the picture below:</p> <p><img src="/assets/img/s11.df997f98.jpg" alt="Image of tables atop Beo Crescent Hawker Centre"></p> <p>This allows us to easily tag images and detect objects based on our classification labels of what:</p> <ul><li>a &quot;Human/Person&quot; is</li> <li>&quot;Crockeries&quot; on the table, and;</li> <li>&quot;Trays&quot; on the table</li></ul> <p>Below is a quick animation of how we trained our Cloud Vision AI model:</p> <p><img src="/assets/img/training.1ac6c563.gif" alt="Training of objects"></p> <h2 id="initialising-tablevision"><a href="#initialising-tablevision" class="header-anchor">#</a> Initialising Tablevision</h2> <p>In order to accurately detect our table boundaries and ID-tag it accordingly (i.e. Table 46, Table 47, etc...), we needed a way to provide coordinates for our algorithm to detect whether a particular table is occupied, or has uncleared trays.</p> <p>For this, we leveraged on the <code>matplotlib</code> Python library to provide the X-y coordinates of our boundaries. We used the <code>widgets.RectangleSelector</code> function to provide us with the 4-point coordinates of the rectangle of our boundary.</p> <p><img src="/assets/img/initialiser.618f18ac.gif" alt="initialiser"></p> <p>After initialising, we send our table coordinates to our <em>Processer</em>. <em>Processer</em> will create <code>Table</code> objects with the coordinates specified.</p> <h2 id="sensor-modalities"><a href="#sensor-modalities" class="header-anchor">#</a> Sensor modalities</h2> <h3 id="defining-session-states"><a href="#defining-session-states" class="header-anchor">#</a> Defining session states</h3> <table><thead><tr><th>State</th> <th>Crocker(ies)</th> <th>People</th> <th>Meaning</th></tr></thead> <tbody><tr><td>0</td> <td>❌</td> <td>❌</td> <td>Vacant</td></tr> <tr><td>1</td> <td>✅</td> <td>❌</td> <td>Vacant but uncleared table</td></tr> <tr><td>2</td> <td>✅</td> <td>✅</td> <td>Occupied</td></tr></tbody></table> <p>We used the table states to detect each session and the activities. In summary, we could simply differentiate the behaviour by:</p> <ul><li><strong>Positive tray-returns</strong>, or self-returns
<ul><li>Table state <strong>0 -&gt; 2 -&gt; 0</strong></li></ul></li> <li><strong>Negative tray-returns</strong>, or cleaner-cleared trays
<ul><li>Table state <strong>0 -&gt; 2 -&gt; 1 -&gt; 0</strong></li></ul></li></ul> <h3 id="processing-the-logic"><a href="#processing-the-logic" class="header-anchor">#</a> Processing the logic</h3> <p>The <code>tablevision_processer.py</code> script, or what we call, <em>The Processor</em>, is essentially an API endpoint that aims to decouple the heavy processing logic away from the low-compute power of the Raspberry Pi, which was deployed together with our camera module at Beo Crescent.</p> <p>The <em>Processor</em> was deployed on the Cloud using a basic Compute instance (in our case, AWS EC2). In this <em>instance</em> (pun intended), it might be an overkill to use an entire EC2, or opted for something like Firebase instead. However, we just wanted something quick and familiar while we worry about getting the right data and insights.</p> <p>The <em>Processor</em> acts as a Cloud API endpoint to receive our images from the Raspberry Pi. It then sends an API request to our Google Vision AI deployed model.</p> <p>An example of the result returned by the Google Vision API is, after formatting by our <code>resultFormatter(prediction)</code> in <em>The Processer</em>:</p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">def</span> <span class="token function">resultFormatter</span><span class="token punctuation">(</span>prediction<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># ... processing the format of prediction here ...</span>

    <span class="token comment"># centre is used to map out the average location of an object. </span>
    <span class="token comment"># This is for us to detect if the object is roughly within the boundaries of a table.</span>
    centre <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;x&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token builtin">min</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">&quot;y&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token builtin">min</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">max</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">}</span>

    <span class="token keyword">return</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;Tray&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;location_boundary&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;x&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0.03948170731707307</span><span class="token punctuation">,</span> <span class="token number">0.4337906504065041</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token string">&quot;y&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0.09692911255411243</span><span class="token punctuation">,</span> <span class="token number">0.09692911255411243</span><span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">&quot;score&quot;</span><span class="token punctuation">:</span> <span class="token number">0.95</span><span class="token punctuation">,</span>
        <span class="token string">&quot;centre_point&quot;</span><span class="token punctuation">:</span> centre
    <span class="token punctuation">}</span>
</code></pre></div><p>Essentially, in a short and sweet summary, our <em>Processer</em> API endpoint does the following:</p> <ol><li>Receives image from our Raspberry Pi deployed on-site</li> <li>Sends the image to Google Cloud Vision AI API</li> <li>Receives object prediction results from Vision AI API</li> <li>Loop through our defined tables</li> <li>Is the object within a table's boundaries?
<ul><li>If the coordinates are within a table's boundaries, update the state of the particular table</li> <li>If no, don't do anything</li></ul></li></ol> <div class="custom-block warning"><p class="custom-block-title">Managing limitations in automated data collection</p> <p>There are some measures put in place to prevent &quot;unclean&quot; data and limitations to the solution.</p> <p>For example, if there are transient people (such as a cleaner entering the table frame to clear crockeries and trays), it might be detected as <strong>Occupied</strong>. So for this, we created a <strong>minimum session timing of 3 minutes</strong>, or 180 seconds.</p></div> <h2 id="data-insights-and-accuracy"><a href="#data-insights-and-accuracy" class="header-anchor">#</a> Data, insights and accuracy</h2> <h3 id="data"><a href="#data" class="header-anchor">#</a> Data</h3> <p>The camera module for Tablevision is connected to the Raspberry Pi. The Pi is used as a gateway node to send image data to our <em>Processer</em>, which processes logic and sends the table state data to our MongoDB databas. An example of a table session data is as follows:</p> <div class="language-json extra-class"><pre class="language-json"><code><span class="token punctuation">{</span>
    <span class="token property">&quot;_id&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
        <span class="token property">&quot;$oid&quot;</span><span class="token operator">:</span> <span class="token string">&quot;5f9386a54c5857a2a68e16e9&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token property">&quot;sessionId&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
        <span class="token property">&quot;$binary&quot;</span><span class="token operator">:</span> <span class="token string">&quot;DhX1DPJkR/O+gBx+e+BrJQ==&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;$type&quot;</span><span class="token operator">:</span> <span class="token string">&quot;3&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token property">&quot;sessionStart&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
        <span class="token property">&quot;$date&quot;</span><span class="token operator">:</span> <span class="token string">&quot;2020-10-24T01:43:01.630Z&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token property">&quot;states&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token property">&quot;tableId&quot;</span><span class="token operator">:</span> <span class="token number">46</span><span class="token punctuation">,</span>
    <span class="token property">&quot;sessionEnd&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
        <span class="token property">&quot;$date&quot;</span><span class="token operator">:</span> <span class="token string">&quot;2020-10-24T01:59:53.236Z&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token property">&quot;tray_count&quot;</span><span class="token operator">:</span> <span class="token number">2</span>
<span class="token punctuation">}</span>
</code></pre></div><p>From the above data entry, notice the <code>states</code> attribute of <code>[0, 2, 0]</code>. The state has gone from <strong>0 (vacant) -&gt; 2 (occupied) -&gt; 0 (vacant)</strong>, which indicates that a patron self-return has occurred.</p> <h2 id="limitations"><a href="#limitations" class="header-anchor">#</a> Limitations</h2> <h3 id="immediate-swap-of-patrons"><a href="#immediate-swap-of-patrons" class="header-anchor">#</a> Immediate swap of patrons</h3> <p>During peak hours, patrons might quickly swap with one another, as seen in the below illustration:</p> <p><img src="/assets/img/swap_patron.09e46557.gif" alt="Patron Swap"></p> <p>In such instances; as our camera only sends image data every ~2-3 seconds, the camera is not able to detect such state changes. This might result in data collection inaccuracies.</p> <p>However, we considered the following statements to counter this limitation:</p> <ul><li>As per the cleaning shift supervisor's directives, trays cannot be cleared when the patron who ate the meal with the tray is still sitting down</li> <li>Patrons who are seated in an uncleared table will ask the cleaners to kindly clear the table for them</li></ul> <p>With these statements, one might be able to implement, using the following pseudocode:</p> <div class="language- extra-class"><pre class="language-text"><code>if Current State is 2:
    if table without Crockeries or Trays:
        then begin new session
        set previous session to 0 &gt; 2 &gt; 1 &gt; 0
</code></pre></div><p><strong>NOTE</strong>: <em>This means that cleaner has cleared the tray, indicating the negative tray return behaviour of the previous patron.</em></p> <h2 id="key-benefits-of-tablevision"><a href="#key-benefits-of-tablevision" class="header-anchor">#</a> Key Benefits of Tablevision</h2> <p>Tablevision might help to benefit the data collection process of tray return behaviour by:</p> <ul><li>identifying tray return behaviour through automated object detection</li> <li>cheaper than deploying manual surveyors in the long run</li> <li>data is digitised, processes are digitalised</li> <li>scale to needs and new functionalities</li></ul> <h3 id="identifying-tray-return-behaviour-automating-data-collection"><a href="#identifying-tray-return-behaviour-automating-data-collection" class="header-anchor">#</a> Identifying tray return behaviour &amp; automating data collection</h3> <p>Leveraging on AI to automatically detect objects, we are able to process logic to understand tray return behaviour. This reduces the need to deploy surveyors as manual labour, and reduces on costs to deploy staff as tray return ambassadors/surveyors.</p> <h3 id="tablevision-digitalises-data-collection"><a href="#tablevision-digitalises-data-collection" class="header-anchor">#</a> Tablevision digitalises data collection</h3> <p>With Tablevision, it's not just digitising the data we collect on tray return behaviour, but it digitalises the way data collection processes are done.</p> <h3 id="scale-to-needs-and-new-functionalities"><a href="#scale-to-needs-and-new-functionalities" class="header-anchor">#</a> Scale to needs and new functionalities</h3> <p>With Tablevision's object boundary functionality, one can rapidly add more table boundaries through initialising Tablevision. Furthermore, the field-of-view of Tablevision is only limited to the infrastructure that Tablevision is deployed at.</p> <p>If one is able to deploy a large camera sensor with a wide-angle lens – or a higher location, Tablevision can be specified to include more tables.</p> <p>More functionalities can be added to Tablevision, such as trash and waste detection, easily. This only requires one to train the custom model for Tablevision.</p> <h2 id="costs"><a href="#costs" class="header-anchor">#</a> Costs</h2> <h3 id="hardware"><a href="#hardware" class="header-anchor">#</a> Hardware</h3> <table><thead><tr><th>Product</th> <th>Price (SGD)</th></tr></thead> <tbody><tr><td>Sony IMX219 Raspberry Pi 8MP Camera Module</td> <td><strong>$ 42</strong></td></tr> <tr><td>Raspberry Pi 3b+ (Provided by the SMU-X Faculty)</td> <td><strong>$ ~50<sup>?</sup></strong></td></tr></tbody></table> <p>Honestly, any camera that's greater than 2MP will do, which includes ordinary web cameras connected through the USB port of the Raspberry Pi.</p> <p>We did not include the costs of using mobile networks through our internet hotspot as this is a proof-of-concept. A fully production-ready deployment would have accounted for networking infrastructure such as the fibre network connectivity.</p> <h3 id="software-ai"><a href="#software-ai" class="header-anchor">#</a> Software + AI</h3> <div class="custom-block warning"><p class="custom-block-title">P.S.</p> <p>We couldn't have executed this without worry if it weren't for the surprising promotional credits for AutoML Vision API.</p></div> <p>Our fixed cost was used to train the machine learning model on AutoML Vision API. We spent a total of <strong>6.87 hours</strong> to train <strong>51 images</strong> with the 3 labels for <strong>US$ 29.73</strong>.</p> <p>As per November 2020, <a href="https://cloud.google.com/vision/automl/pricing#image_classification_deployment_and_prediction_costs" target="_blank" rel="noopener noreferrer">Google Cloud Vision API<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>'s Pricing page states (in USD):</p> <blockquote><p>The cost for AutoML Vision Image Classification model training is <strong>$3.15 per node hour</strong>.</p> <p>For each unit of time, we use 8 nodes in parallel, where each node is equivalent to a n1-standard-8 machine with an attached NVIDIA® Tesla® V100 GPU.</p></blockquote> <blockquote><p>The cost for deployment and prediction is <strong>$1.25 per node hour</strong>. One node is usually sufficient for most experimental traffic.</p></blockquote> <p>The main disadvantage of using the AutoML Vision AI is that the compute instances used to deploy our models doesn't automatically scale down or up when needed. This will result in underutilised model deployments and is not cost effective at all.</p> <p>If we were to deploy for 14 hours with one node (our current configuration for the proof-of-concept) – the length of Beo Crescent Market Food Centre's operating hours, that will cost <strong>S$ 35</strong> per operational day (taken from our billing statement in SGD)</p> <table><thead><tr><th>Product</th> <th>Price (SGD)</th> <th>Units</th></tr></thead> <tbody><tr><td>AutoML Model Training</td> <td><strong>$ 29.73</strong></td> <td>One-time</td></tr> <tr><td>Prediction request deployment</td> <td><strong>$ 35</strong></td> <td>per day</td></tr></tbody></table> <p>We discovered that one could deploy the AutoML model on Cloud Run, which is essentially Google's answer to AWS Lambda – Functions-as-a-Service (FaaS). However, due to the limited timeframe of the project (and partly, due to the free credits 😊 ), we did not deploy the model on Cloud Run.</p> <p>If you're interested, this <a href="https://medium.com/@juri.sarbach/how-to-deploy-your-automl-model-in-a-cost-effective-way-5efdd377d4d2" target="_blank" rel="noopener noreferrer">Medium article by Juri Sarbach<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> walks the reader through on how to export the AutoML Model to Google Cloud Storage (GCS), and then to Cloud Run.</p> <p>However, Juri pointed out the tradeoff with using Cloud Run:</p> <blockquote><p><em>Typically, scale-to-zero services suffer from cold start latency when they haven’t been used for some time.</em></p></blockquote> <p>In short, if we were to deploy on Cloud Run, here's the quick maths behind it:</p> <ul><li>Our requests come in once every 2 seconds, or about <strong>420 requests per day</strong></li></ul> <p>From the <a href="https://cloud.google.com/run/pricing" target="_blank" rel="noopener noreferrer">Google Cloud Run Pricing page<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>:</p> <ul><li>2 million requests free per month</li> <li>US$ 0.40 / million requests beyond the free quota</li></ul> <p>Then, for the sake of simple calculations:</p> <table><thead><tr><th>Product</th> <th>Price (SGD)</th> <th>Units</th></tr></thead> <tbody><tr><td>AutoML Model Training</td> <td><strong>$ 42</strong></td> <td>One-time</td></tr> <tr><td>Cloud Run deployment</td> <td><strong>FREE<sup>1</sup></strong></td> <td>per month</td></tr></tbody></table> <p><em><sup>1</sup> For the sake of quick calculations, not considering networking costs incurred by transferring images across the mobile network (mobile hotspot), our Cloud Run deployment should be nearly free.</em></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/journal/fsr-rfid.html" class="prev">
        Round Force Sensor + RFID Tray Tagging
      </a></span> <!----></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.438a13a4.js" defer></script><script src="/assets/js/2.8c709f75.js" defer></script><script src="/assets/js/4.31391edf.js" defer></script>
  </body>
</html>
