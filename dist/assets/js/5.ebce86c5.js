(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{394:function(t,e,a){t.exports=a.p+"assets/img/tablevision.81a39ea1.png"},395:function(t,e,a){t.exports=a.p+"assets/img/s11.df997f98.jpg"},396:function(t,e,a){t.exports=a.p+"assets/img/training.1ac6c563.gif"},397:function(t,e,a){t.exports=a.p+"assets/img/initialiser.618f18ac.gif"},398:function(t,e,a){t.exports=a.p+"assets/img/accuracy1.2cdaa114.jpg"},399:function(t,e,a){t.exports=a.p+"assets/img/accuracy2.6415956e.jpg"},400:function(t,e,a){t.exports=a.p+"assets/img/swap_patron.09e46557.gif"},408:function(t,e,a){"use strict";a.r(e);var s=a(42),r=Object(s.a)({},(function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"tablevision-tray-returns-monitoring-with-ai"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tablevision-tray-returns-monitoring-with-ai"}},[t._v("#")]),t._v(" Tablevision: Tray Returns Monitoring with AI")]),t._v(" "),s("h2",{attrs:{id:"architecture-diagram"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#architecture-diagram"}},[t._v("#")]),t._v(" Architecture Diagram")]),t._v(" "),s("p",[s("img",{attrs:{src:a(394),alt:"diagram"}})]),t._v(" "),s("h2",{attrs:{id:"overview"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#overview"}},[t._v("#")]),t._v(" Overview")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("NOTE")]),t._v(" "),s("p",[t._v('The term "tray returns" is our umbrella term that refers to hawker centre trays or crockeries (i.e. bowls, cutleries, plates) returned to the Tray Return Point.')])]),t._v(" "),s("p",[t._v("Our solution incorporates an 8-megapixel camera sensor, connected to a sensor gateway. In our case, the gateway is our Raspberry Pi. It is designed to be able to automate the detection of self-initiated tray returns – or the reverse of tray returns by cleaners, through a custom machine learning model training.")]),t._v(" "),s("p",[t._v("Through Tablevision, we are able to monitor:")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("Negative tray return rates")]),t._v(': a measure of tray returns by the cleaners – hence the term "negative" which indicates negative behaviour by patrons for not clearing trays, and;')]),t._v(" "),s("li",[s("strong",[t._v("Positive tray return rates")]),t._v(': a measure of tray self-returns by patrons – the term "positive" indicates the positive behaviour of patrons clearing up after eating')])]),t._v(" "),s("h2",{attrs:{id:"training-our-machine-learning-model"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#training-our-machine-learning-model"}},[t._v("#")]),t._v(" Training our Machine Learning Model")]),t._v(" "),s("p",[t._v("Initially, we used the "),s("a",{attrs:{href:"https://www.clarifai.com/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Clarif.ai API"),s("OutboundLink")],1),t._v(" for its object detection API. However, the free version is only available for 1,000 API calls, which was insufficient for our use case. Furthermore, we needed a pay-as-you-go (PAYG) model that enables us to have a fine-grained billing for our object detection, and Clarif.ai's pricing model only indicates prices per block of API calls.")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://cloud.google.com/vision",target:"_blank",rel:"noopener noreferrer"}},[t._v("Google Cloud Vision AI"),s("OutboundLink")],1),t._v(" comes in as our next reliable alternative. It enables us to derive and detect objects from the image captured with our camera modules, and it comes with a pay-as-you-go model.")]),t._v(" "),s("p",[t._v("We decided to give it a try.")]),t._v(" "),s("p",[t._v("And that's when we realised that Google had generously given us free $300 credits for model training and image detection, on top of the $300 credit already given as a promotion to use any of Google Cloud's services. Such a great way to let us students experiment with Cloud technologies.")]),t._v(" "),s("p",[t._v("To train our model, we used a series of images that we captured from our installed camera module atop some tables, such as the picture below:")]),t._v(" "),s("p",[s("img",{attrs:{src:a(395),alt:"Image of tables atop Beo Crescent Hawker Centre"}})]),t._v(" "),s("p",[t._v("This allows us to easily tag images and detect objects based on our classification labels of what:")]),t._v(" "),s("ul",[s("li",[t._v('a "Human/Person" is')]),t._v(" "),s("li",[t._v('"Crockeries" on the table, and;')]),t._v(" "),s("li",[t._v('"Trays" on the table')])]),t._v(" "),s("p",[t._v("Below is a quick animation of how we trained our Cloud Vision AI model:")]),t._v(" "),s("p",[s("img",{attrs:{src:a(396),alt:"Training of objects"}})]),t._v(" "),s("h2",{attrs:{id:"initialising-tablevision"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#initialising-tablevision"}},[t._v("#")]),t._v(" Initialising Tablevision")]),t._v(" "),s("p",[t._v("In order to accurately detect our table boundaries and ID-tag it accordingly (i.e. Table 46, Table 47, etc...), we needed a way to provide coordinates for our algorithm to detect whether a particular table is occupied, or has uncleared trays.")]),t._v(" "),s("p",[t._v("For this, we leveraged on the "),s("code",[t._v("matplotlib")]),t._v(" Python library to provide the X-y coordinates of our boundaries. We used the "),s("code",[t._v("widgets.RectangleSelector")]),t._v(" function to provide us with the 4-point coordinates of the rectangle of our boundary.")]),t._v(" "),s("p",[s("img",{attrs:{src:a(397),alt:"initialiser"}})]),t._v(" "),s("p",[t._v("After initialising, we will send our table coordinates to our "),s("em",[t._v("Processer")]),t._v(". "),s("em",[t._v("Processer")]),t._v(" will create "),s("code",[t._v("Table")]),t._v(" objects with the coordinates specified.")]),t._v(" "),s("h2",{attrs:{id:"sensor-modalities"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#sensor-modalities"}},[t._v("#")]),t._v(" Sensor modalities")]),t._v(" "),s("h3",{attrs:{id:"defining-session-states"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#defining-session-states"}},[t._v("#")]),t._v(" Defining session states")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("State")]),t._v(" "),s("th",[t._v("Crocker(ies)")]),t._v(" "),s("th",[t._v("People")]),t._v(" "),s("th",[t._v("Meaning")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("0")]),t._v(" "),s("td",[t._v("❌")]),t._v(" "),s("td",[t._v("❌")]),t._v(" "),s("td",[t._v("Vacant")])]),t._v(" "),s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("✅")]),t._v(" "),s("td",[t._v("❌")]),t._v(" "),s("td",[t._v("Vacant but uncleared table")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("✅")]),t._v(" "),s("td",[t._v("✅")]),t._v(" "),s("td",[t._v("Occupied")])])])]),t._v(" "),s("p",[t._v("We used the table states to detect each session and the activities. In summary, we could simply differentiate the behaviour by:")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("Positive tray-returns")]),t._v(", or self-returns\n"),s("ul",[s("li",[t._v("Table state "),s("strong",[t._v("0 -> 2 -> 0")])])])]),t._v(" "),s("li",[s("strong",[t._v("Negative tray-returns")]),t._v(", or cleaner-cleared trays\n"),s("ul",[s("li",[t._v("Table state "),s("strong",[t._v("0 -> 2 -> 1 -> 0")])])])])]),t._v(" "),s("h3",{attrs:{id:"processing-the-logic"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#processing-the-logic"}},[t._v("#")]),t._v(" Processing the logic")]),t._v(" "),s("p",[t._v("The "),s("code",[t._v("tablevision_processer.py")]),t._v(" script, or what we call, "),s("em",[t._v("The Processor")]),t._v(", is essentially an API endpoint that aims to decouple the heavy processing logic away from the low-compute power of the Raspberry Pi, which was deployed together with our camera module at Beo Crescent.")]),t._v(" "),s("p",[t._v("The "),s("em",[t._v("Processor")]),t._v(" was deployed on the Cloud using a basic Compute instance (in our case, AWS EC2). In this "),s("em",[t._v("instance")]),t._v(" (pun intended), it might be an overkill to use an entire EC2, and we could've opted for something like Firebase instead. However, we just wanted something quick and familiar while we worry about getting the right data and insights.")]),t._v(" "),s("p",[t._v("The "),s("em",[t._v("Processor")]),t._v(" acts as a Cloud API endpoint to receive our images from the Raspberry Pi. It then sends an API request to our Google Vision AI deployed model.")]),t._v(" "),s("p",[t._v("An example of the result returned by the Google Vision API is, after formatting by our "),s("code",[t._v("resultFormatter(prediction)")]),t._v(" in "),s("em",[t._v("The Processer")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("resultFormatter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("prediction"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ... processing the format of prediction here ...")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# centre is used to map out the average location of an object. ")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# This is for us to detect if the object is roughly within the boundaries of a table.")]),t._v("\n    centre "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"x"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("min")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"y"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("min")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Tray"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"location_boundary"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"x"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.03948170731707307")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4337906504065041")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"y"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.09692911255411243")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.09692911255411243")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"score"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.95")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"centre_point"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" centre\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("Essentially, in a short and sweet summary, our "),s("em",[t._v("Processer")]),t._v(" API endpoint does the following:")]),t._v(" "),s("ol",[s("li",[t._v("Receives image from our Raspberry Pi deployed on-site")]),t._v(" "),s("li",[t._v("Sends the image to Google Cloud Vision AI API")]),t._v(" "),s("li",[t._v("Receives object prediction results from Vision AI API")]),t._v(" "),s("li",[t._v("Loop through our defined tables")]),t._v(" "),s("li",[t._v("Is the object within a table's boundaries?\n"),s("ul",[s("li",[t._v("If the coordinates are within a table's boundaries, update the state of the particular table")]),t._v(" "),s("li",[t._v("If no, don't do anything")])])])]),t._v(" "),s("div",{staticClass:"custom-block warning"},[s("p",{staticClass:"custom-block-title"},[t._v("Managing limitations in automated data collection")]),t._v(" "),s("p",[t._v('There are some measures put in place to prevent "unclean" data and limitations to the solution.')]),t._v(" "),s("p",[t._v("For example, if there are transient people (such as a cleaner entering the table frame to clear crockeries and trays), it might be detected as "),s("strong",[t._v("Occupied")]),t._v(". So for this, we created a "),s("strong",[t._v("minimum session timing of 3 minutes")]),t._v(", or 180 seconds.")])]),t._v(" "),s("h2",{attrs:{id:"data-insights-and-accuracy"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-insights-and-accuracy"}},[t._v("#")]),t._v(" Data, Insights and Accuracy")]),t._v(" "),s("h3",{attrs:{id:"data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data"}},[t._v("#")]),t._v(" Data")]),t._v(" "),s("p",[t._v("The camera module for Tablevision is connected to the Raspberry Pi. The Pi is used as a gateway node to send image data to our "),s("em",[t._v("Processer")]),t._v(", which processes logic and sends the table state data to our MongoDB databas. An example of a table session data is as follows:")]),t._v(" "),s("div",{staticClass:"language-json extra-class"},[s("pre",{pre:!0,attrs:{class:"language-json"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"_id"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"$oid"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"5f9386a54c5857a2a68e16e9"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"sessionId"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"$binary"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"DhX1DPJkR/O+gBx+e+BrJQ=="')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"$type"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"3"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"sessionStart"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"$date"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2020-10-24T01:43:01.630Z"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"states"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"tableId"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("46")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"sessionEnd"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"$date"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2020-10-24T01:59:53.236Z"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"tray_count"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("From the above data entry, notice the "),s("code",[t._v("states")]),t._v(" attribute of "),s("code",[t._v("[0, 2, 0]")]),t._v(". The state has gone\nfrom "),s("strong",[t._v("0 (vacant) -> 2 (occupied) -> 0 (vacant)")]),t._v(", which indicates that a patron self-return has occurred.")]),t._v(" "),s("h3",{attrs:{id:"insights"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#insights"}},[t._v("#")]),t._v(" Insights")]),t._v(" "),s("p",[t._v("Being just a proof-of-concept, we deployed the camera over a span of 4 full days (27 Oct 2020 to 30 Oct 2020).")]),t._v(" "),s("p",[t._v("As we were able to utilise and make full use of our trained model for detecting objects, we were able to log down the number of trays that is on the table per session. With data on the number of trays available, we could generate useful insights on the patrons' tray usage behavior. Below is the distribution frequency for the positive & negative tray return rates grouped by the number of trays.")]),t._v(" "),s("p",[s("strong",[t._v("Positive Tray Returns (Self-returns)")])]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("No. of Trays")]),t._v(" "),s("th",[t._v("Cumulative Distribution (%)")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("45.44%")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("32.34%")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("9.72%")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("12.5%")])])])]),t._v(" "),s("p",[s("strong",[t._v("Negative Tray Returns (Cleaner Returns)")])]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("No. of Trays")]),t._v(" "),s("th",[t._v("Cumulative Distribution (%)")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("1")]),t._v(" "),s("td",[t._v("17.52%")])]),t._v(" "),s("tr",[s("td",[t._v("2")]),t._v(" "),s("td",[t._v("17.88%")])]),t._v(" "),s("tr",[s("td",[t._v("3")]),t._v(" "),s("td",[t._v("24.09%")])]),t._v(" "),s("tr",[s("td",[t._v("4")]),t._v(" "),s("td",[t._v("40.51%")])])])]),t._v(" "),s("p",[t._v("Hence, we were able to deduce the following:")]),t._v(" "),s("ol",[s("li",[t._v("For "),s("strong",[t._v("Positive Self Return")]),t._v(", most patron will return their own tray if the tray count is less than 3")]),t._v(" "),s("li",[t._v("For "),s("strong",[t._v("Negative Cleaner Return")]),t._v(", most patron does not return their own tray if the tray count is more than 2")])]),t._v(" "),s("h3",{attrs:{id:"accuracy"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#accuracy"}},[t._v("#")]),t._v(" Accuracy")]),t._v(" "),s("h4",{attrs:{id:"ground-truth-vs-sensor-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#ground-truth-vs-sensor-data"}},[t._v("#")]),t._v(" Ground Truth vs Sensor Data")]),t._v(" "),s("p",[s("img",{attrs:{src:a(398),alt:"ground truth vs sensor"}})]),t._v(" "),s("p",[t._v("To identify the accuracy of the data collected, our team collected and compared the ground truth data against data collected from the IoT devices. With reference to the above figure, we used the Percent Error formula to calculate the accuracy of our data. The data accuracy for Tablevision is at "),s("strong",[t._v("46.15%")]),t._v(".")]),t._v(" "),s("p",[t._v("To calculate percentage error, we use this "),s("a",{attrs:{href:"https://www.omnicalculator.com/statistics/accuracy#how-to-use-the-accuracy-calculator",target:"_blank",rel:"noopener noreferrer"}},[t._v("formula"),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Percent error")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("(|(Vo - Vₐ)|/Vₐ) * 100")])])])]),t._v(" "),s("h4",{attrs:{id:"stats"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#stats"}},[t._v("#")]),t._v(" Stats")]),t._v(" "),s("p",[t._v("To further validate the data accuracy, we ran a T-Test to compare any significant differences between MSE and our dataset.")]),t._v(" "),s("p",[s("img",{attrs:{src:a(399),alt:"t-test"}})]),t._v(" "),s("p",[t._v("From the calculation, we are 95% confident that the average self-returned trays will fall within the range of 51%")]),t._v(" "),s("h2",{attrs:{id:"limitations"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#limitations"}},[t._v("#")]),t._v(" Limitations")]),t._v(" "),s("h3",{attrs:{id:"immediate-swap-of-patrons"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#immediate-swap-of-patrons"}},[t._v("#")]),t._v(" Immediate swap of patrons")]),t._v(" "),s("p",[t._v("During peak hours, patrons might quickly swap with one another, as seen in the below illustration:")]),t._v(" "),s("p",[s("img",{attrs:{src:a(400),alt:"Patron Swap"}})]),t._v(" "),s("p",[t._v("In such instances; as our camera only sends image data every ~2-3 seconds, the camera is not able to detect such state changes. This might result in data collection inaccuracies.")]),t._v(" "),s("p",[t._v("However, we considered the following statements to counter this limitation:")]),t._v(" "),s("ul",[s("li",[t._v("As per the cleaning shift supervisor's directives, trays cannot be cleared when the patron who ate the meal with the tray is still sitting down")]),t._v(" "),s("li",[t._v("Patrons who are seated in an uncleared table will ask the cleaners to kindly clear the table for them")])]),t._v(" "),s("p",[t._v("With these statements, one might be able to implement, using the following pseudocode:")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("if Current State is 2:\n    if table without Crockeries or Trays:\n        then begin new session\n        set previous session to 0 > 2 > 1 > 0\n")])])]),s("p",[s("strong",[t._v("NOTE")]),t._v(": "),s("em",[t._v("This means that cleaner has cleared the tray, indicating the negative tray return behaviour of the previous patron.")])]),t._v(" "),s("h2",{attrs:{id:"key-benefits-of-tablevision"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#key-benefits-of-tablevision"}},[t._v("#")]),t._v(" Key Benefits of Tablevision")]),t._v(" "),s("p",[t._v("Tablevision might help to benefit the data collection process of tray return behaviour by:")]),t._v(" "),s("ul",[s("li",[t._v("identifying tray return behaviour through automated object detection")]),t._v(" "),s("li",[t._v("cheaper than deploying manual surveyors in the long run")]),t._v(" "),s("li",[t._v("data is digitised, processes are digitalised")]),t._v(" "),s("li",[t._v("scale to needs and new functionalities")])]),t._v(" "),s("h3",{attrs:{id:"identifying-tray-return-behaviour-automating-data-collection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#identifying-tray-return-behaviour-automating-data-collection"}},[t._v("#")]),t._v(" Identifying tray return behaviour & automating data collection")]),t._v(" "),s("p",[t._v("Leveraging on AI to automatically detect objects, we are able to process logic to understand tray return behaviour. This reduces the need to deploy surveyors as manual labour, and reduces on costs to deploy staff as tray return ambassadors/surveyors.")]),t._v(" "),s("h3",{attrs:{id:"tablevision-digitalises-data-collection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tablevision-digitalises-data-collection"}},[t._v("#")]),t._v(" Tablevision digitalises data collection")]),t._v(" "),s("p",[t._v("With Tablevision, it's not just digitising the data we collect on tray return behaviour, but it digitalises the way data collection processes are done.")]),t._v(" "),s("h3",{attrs:{id:"scale-to-needs-and-new-functionalities"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#scale-to-needs-and-new-functionalities"}},[t._v("#")]),t._v(" Scale to needs and new functionalities")]),t._v(" "),s("p",[t._v("With Tablevision's object boundary functionality, one can rapidly add more table boundaries through initialising Tablevision. Furthermore, the field-of-view of Tablevision is only limited to the infrastructure that Tablevision is deployed at.")]),t._v(" "),s("p",[t._v("If one is able to deploy a large camera sensor with a wide-angle lens – or a higher location, Tablevision can be specified to include more tables.")]),t._v(" "),s("p",[t._v("More functionalities can be added to Tablevision, such as trash and waste detection, easily. This only requires one to train the custom model for Tablevision.")]),t._v(" "),s("h2",{attrs:{id:"costs"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#costs"}},[t._v("#")]),t._v(" Costs")]),t._v(" "),s("h3",{attrs:{id:"hardware"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hardware"}},[t._v("#")]),t._v(" Hardware")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Product")]),t._v(" "),s("th",[t._v("Price (SGD)")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("Sony IMX219 Raspberry Pi 8MP Camera Module")]),t._v(" "),s("td",[s("strong",[t._v("$ 42")])])]),t._v(" "),s("tr",[s("td",[t._v("Raspberry Pi 3b+ (Provided by the SMU-X Faculty)")]),t._v(" "),s("td",[s("strong",[t._v("$ ~50"),s("sup",[t._v("?")])])])])])]),t._v(" "),s("p",[t._v("Honestly, any camera that's greater than 2MP will do, which includes ordinary web cameras connected through the USB port of the Raspberry Pi.")]),t._v(" "),s("p",[t._v("We did not include the costs of using mobile networks through our internet hotspot as this is a proof-of-concept. A fully production-ready deployment would have accounted for networking infrastructure such as the fibre network connectivity.")]),t._v(" "),s("h3",{attrs:{id:"software-ai"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#software-ai"}},[t._v("#")]),t._v(" Software + AI")]),t._v(" "),s("div",{staticClass:"custom-block warning"},[s("p",{staticClass:"custom-block-title"},[t._v("P.S.")]),t._v(" "),s("p",[t._v("We couldn't have executed this without worry if it weren't for the surprising promotional credits for AutoML Vision API.")])]),t._v(" "),s("p",[t._v("Our fixed cost was used to train the machine learning model on AutoML Vision API. We spent a total of "),s("strong",[t._v("6.87 hours")]),t._v(" to train "),s("strong",[t._v("51 images")]),t._v(" with the 3 labels for "),s("strong",[t._v("US$ 29.73")]),t._v(".")]),t._v(" "),s("p",[t._v("As per November 2020, "),s("a",{attrs:{href:"https://cloud.google.com/vision/automl/pricing#image_classification_deployment_and_prediction_costs",target:"_blank",rel:"noopener noreferrer"}},[t._v("Google Cloud Vision API"),s("OutboundLink")],1),t._v("'s Pricing page states (in USD):")]),t._v(" "),s("blockquote",[s("p",[t._v("The cost for AutoML Vision Image Classification model training is "),s("strong",[t._v("$3.15 per node hour")]),t._v(".")]),t._v(" "),s("p",[t._v("For each unit of time, we use 8 nodes in parallel, where each node is equivalent to a n1-standard-8 machine with an attached NVIDIA® Tesla® V100 GPU.")])]),t._v(" "),s("blockquote",[s("p",[t._v("The cost for deployment and prediction is "),s("strong",[t._v("$1.25 per node hour")]),t._v(". One node is usually sufficient for most experimental traffic.")])]),t._v(" "),s("p",[t._v("The main disadvantage of using the AutoML Vision AI is that the compute instances used to deploy our models doesn't automatically scale down or up when needed. This will result in underutilised model deployments and is not cost effective at all.")]),t._v(" "),s("p",[t._v("If we were to deploy for 14 hours with one node (our current configuration for the proof-of-concept) – the length of Beo Crescent Market Food Centre's operating hours, that will cost "),s("strong",[t._v("S$ 35")]),t._v(" per operational day (taken from our billing statement in SGD)")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Product")]),t._v(" "),s("th",[t._v("Price (SGD)")]),t._v(" "),s("th",[t._v("Units")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("AutoML Model Training")]),t._v(" "),s("td",[s("strong",[t._v("$ 29.73")])]),t._v(" "),s("td",[t._v("One-time")])]),t._v(" "),s("tr",[s("td",[t._v("Prediction request deployment")]),t._v(" "),s("td",[s("strong",[t._v("$ 35")])]),t._v(" "),s("td",[t._v("per day")])])])]),t._v(" "),s("p",[t._v("We discovered that one could deploy the AutoML model on Cloud Run, which is essentially Google's answer to AWS Lambda – Functions-as-a-Service (FaaS). However, due to the limited timeframe of the project (and partly, due to the free credits 😊 ), we did not deploy the model on Cloud Run.")]),t._v(" "),s("p",[t._v("If you're interested, this "),s("a",{attrs:{href:"https://medium.com/@juri.sarbach/how-to-deploy-your-automl-model-in-a-cost-effective-way-5efdd377d4d2",target:"_blank",rel:"noopener noreferrer"}},[t._v("Medium article by Juri Sarbach"),s("OutboundLink")],1),t._v(" walks the reader through on how to export the AutoML Model to Google Cloud Storage (GCS), and then to Cloud Run.")]),t._v(" "),s("p",[t._v("However, Juri pointed out the tradeoff with using Cloud Run:")]),t._v(" "),s("blockquote",[s("p",[s("em",[t._v("Typically, scale-to-zero services suffer from cold start latency when they haven’t been used for some time.")])])]),t._v(" "),s("p",[t._v("In short, if we were to deploy on Cloud Run, here's the quick maths behind it:")]),t._v(" "),s("ul",[s("li",[t._v("Our requests come in once every 2 seconds, or about "),s("strong",[t._v("420 requests per day")])])]),t._v(" "),s("p",[t._v("From the "),s("a",{attrs:{href:"https://cloud.google.com/run/pricing",target:"_blank",rel:"noopener noreferrer"}},[t._v("Google Cloud Run Pricing page"),s("OutboundLink")],1),t._v(":")]),t._v(" "),s("ul",[s("li",[t._v("2 million requests free per month")]),t._v(" "),s("li",[t._v("US$ 0.40 / million requests beyond the free quota")])]),t._v(" "),s("p",[t._v("Then, for the sake of simple calculations:")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Product")]),t._v(" "),s("th",[t._v("Price (SGD)")]),t._v(" "),s("th",[t._v("Units")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("AutoML Model Training")]),t._v(" "),s("td",[s("strong",[t._v("$ 42")])]),t._v(" "),s("td",[t._v("One-time")])]),t._v(" "),s("tr",[s("td",[t._v("Cloud Run deployment")]),t._v(" "),s("td",[s("strong",[t._v("FREE"),s("sup",[t._v("1")])])]),t._v(" "),s("td",[t._v("per month")])])])]),t._v(" "),s("p",[s("em",[s("sup",[t._v("1")]),t._v(" For the sake of quick calculations, not considering networking costs incurred by transferring images across the mobile network (mobile hotspot), our Cloud Run deployment should be nearly free.")])])])}),[],!1,null,null,null);e.default=r.exports}}]);